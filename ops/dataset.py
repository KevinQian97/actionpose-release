import torch.utils.data as data
from avi_r import AVIReader
from PIL import Image
import os
import torch
import numpy as np
from numpy.random import randint
import cv2
import json
import decord
from decord import VideoReader
from decord import cpu, gpu
import torchvision
import gc
import math
from ops.record import VideoRecord_NTU as VideoRecord
decord.bridge.set_bridge('torch')


class TSNDataSet(data.Dataset):
    def __init__(self, root_path, list_file,
                 num_segments=3, new_length=1, modality='RGB',
                 image_tmpl='img_{:05d}.jpg', transform=None,
                 random_shift=True, test_mode=False,
                 remove_missing=False, dense_sample=False, twice_sample=False, all_sample=False, analysis=False):

        self.root_path = root_path
        self.list_file = list_file
        self.num_segments = num_segments
        self.new_length = new_length
        self.modality = modality
        self.image_tmpl = image_tmpl
        self.transform = transform
        self.random_shift = random_shift
        self.test_mode = test_mode
        self.remove_missing = remove_missing
        self.dense_sample = dense_sample  # using dense sample as I3D
        self.twice_sample = twice_sample  # twice sample for more validation
        self.all_sample = all_sample #all sample for r21d on MEVA
        self.analysis = analysis
        if self.dense_sample:
            print('=> Using dense sample for the dataset...')
        if self.twice_sample:
            print('=> Using twice sample for the dataset...')

        if self.modality == 'RGBDiff':
            self.new_length += 1  # Diff needs one more image to calculate diff
        self._parse_list()


# also modify it to video loader
    def _load_image(self, frames, directory, idx, start):
        if self.modality == 'RGB' or self.modality == 'RGBDiff' or self.modality == 'PoseAction':
            try:
                if idx >= len(frames):
                    idx = len(frames)-1
                return [frames[idx]]
            except Exception:
                print('error loading video:{} whose idx is {}'.format(os.path.join(self.root_path, directory),start+idx))
                return [frames[0]]

# parse_list for json input
    def _parse_list(self):
        tmp = json.load(open(self.list_file,"r"))["database"]
        items = tmp.keys()
        self.video_list = [VideoRecord(tmp[item],item,self.root_path) for item in items]
        print('video number:%d' % (len(self.video_list)))

    def _sample_indices(self, record):
        """

        :param record: VideoRecord
        :return: list
        """
        if self.dense_sample:  # i3d dense sample
            sample_pos = max(1, 1 + record.num_frames - 64)
            t_stride = 64 // self.num_segments
            start_idx = 0 if sample_pos == 1 else np.random.randint(0, sample_pos - 1)
            offsets = [(idx * t_stride + start_idx) % record.num_frames for idx in range(self.num_segments)]
            return np.array(offsets)
        if self.all_sample:
            return np.array(range(record.num_frames))
        else:  # normal sample
            average_duration = (record.num_frames - self.new_length + 1) // self.num_segments
            if average_duration > 0:
                offsets = np.multiply(list(range(self.num_segments)), average_duration) + randint(average_duration,
                                                                                                  size=self.num_segments)
            elif record.num_frames > self.num_segments:
                offsets = np.sort(randint(record.num_frames - self.new_length + 1, size=self.num_segments))
            else:
                offsets = np.zeros((self.num_segments))
            return offsets

    def _get_val_indices(self, record):
        if self.dense_sample:  # i3d dense sample
            sample_pos = max(1, 1 + record.num_frames - 64)
            t_stride = 64 // self.num_segments
            start_idx = 0 if sample_pos == 1 else np.random.randint(0, sample_pos - 1)
            offsets = [(idx * t_stride + start_idx) % record.num_frames for idx in range(self.num_segments)]
            return np.array(offsets)
        if self.all_sample:
            return np.array(range(record.num_frames))
        else:
            if record.num_frames > self.num_segments + self.new_length - 1:
                tick = (record.num_frames - self.new_length + 1) / float(self.num_segments)
                offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])
            else:
                offsets = np.zeros((self.num_segments,))
            return offsets

    def _get_test_indices(self, record):
        if self.dense_sample:
            sample_pos = max(1, 1 + record.num_frames - 64)
            t_stride = 64 // self.num_segments
            start_list = np.linspace(0, sample_pos - 1, num=10, dtype=int)
            offsets = []
            for start_idx in start_list.tolist():
                offsets += [(idx * t_stride + start_idx) % record.num_frames for idx in range(self.num_segments)]
            return np.array(offsets)
        if self.all_sample:
            return np.array(range(record.num_frames))
        elif self.twice_sample:
            tick = (record.num_frames - self.new_length + 1) / float(self.num_segments)

            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)] +
                               [int(tick * x) for x in range(self.num_segments)])

            return offsets
        else:
            tick = (record.num_frames - self.new_length + 1) / float(self.num_segments)
            offsets = np.array([int(tick / 2.0 + tick * x)-1 for x in range(self.num_segments)])
            return offsets

    def __getitem__(self, index):
        record = self.video_list[index]

        if not self.test_mode:
            segment_indices = self._sample_indices(record) if self.random_shift else self._get_val_indices(record)
        else:
            segment_indices = self._get_test_indices(record)
        return self.get(record, segment_indices)

# get for json input
    def get(self, record, indices):
        # print(indices)
        images = list()
        frames = record.frames()
        for seg_ind in indices:
            p = int(seg_ind)
            for i in range(self.new_length):
                seg_imgs = self._load_image(frames,record.name,p,record.start)
                images.extend(seg_imgs)
                if p < record.num_frames:
                    p += 1
        # images = torch.stack(images,dim=0).float()
        # for image in images:
        #     print(type(image))
        process_data = self.transform(images)
        if self.modality == "PoseAction":
            skels = torch.FloatTensor(record.joints())
            joints = []
            for seg_ind in indices:
                p = int(seg_ind)
                joints.append(skels[:,p])
            joints = torch.stack(joints,dim=1)
            if not self.analysis:
                return {"vid":process_data,"joints":joints}, record.label
            else:
                return {"vid":process_data,"joints":joints}, record.label, record.path

        else:
            if self.analysis:
                return process_data, record.label, record.path
            else:
                return process_data, record.label

    def __len__(self):
        return len(self.video_list)